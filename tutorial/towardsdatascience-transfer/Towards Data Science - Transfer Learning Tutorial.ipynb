{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Towards Data Science - Transfer Learning Tutorial\n",
        "\n",
        "This is from https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a\n",
        "is the overview of the techniques, but at the end is a practical guide in the real world using Keras to graft a dog and cat discriminator on top of a pretrained model.\n",
        "\n",
        "# A note on using this with docker\n",
        "If you are using a docker container make sure there is enogh emmory. With just 2GB of memory, the file loading with thrash. Suggest 4-6GB. Set this in Docker for Desktop on the Mac in the Preferences/Advanced\n",
        "\n",
        "# Notes on gathering images from Google Images\n",
        "There are a bunch of Chrome download helpers that will download a complete images. The recommended one is called Fatkun Batch downloader. YOu have to remember to ask it to rename the images. And make sure \"Ask where to download is\" *not* set in Preferences\n",
        "\n",
        "There is another called ImageAssistant Batch Loader. both of these work pretty well with the ImageAssistant also harmonizing all the image names\n",
        "\n",
        "# Getting data from kaggle\n",
        "\n",
        "Where we will apply transfer learning to dogs vs cats on top of existing deep neural networks. First we have to get the actual data from kaggle at https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "\n",
        "You can either click on the download link or use a python utility called kaggle to get it from https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "Security-wise, you should goto your account `https://kaggle.com/<username>/account` and select `Create an API Token` whih will generation `kaggle.json` which should live in `~/.kaggle/kaggle.json`. If you are using @Rich's scheme, this means you should copy that to the `/Volumes/rich.vc/.ssh` and this will get linked in the ~/.ssh directory. And then you want a symlink from there `ln -s ~/.ssh/kaggle.json ~/.kaggle` and this will be secure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comment out this line if you really want to run this\n",
        "should only do once\n",
        "# only needs to be installed once\n",
        "!pip install kaggle numpy\n",
        "# make sure to go to https://www.kaggle.com/c/dogs-vs-cats/rules\n",
        "# And accept the rules of the competition\n",
        "# make sure to that git lfs is operating if you do this in a git repo\n",
        "# Run the line below once. Warning this is a 1GB download!\n",
        "kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on loading libraries and dependencies\n",
        "First, let's make sure list in the Jupyter Notebook, the actual configuration we are running using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt\n",
        "!cat requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will give you two files `train.zip` and `test1.zip` with 500MB and 250MB worth of JPEGs. So let's process them. As an aside, you definitely to `git track \"*.jpg\"` before sticking this into your repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# keep things deterministic and set a fixed seed\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now use glob to expand the files names in the train directory. then use an expression that sticks all the files names in cat_files and dog_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files = glob.glob('train/*')\n",
        "print('total files', len(files), files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_files = [f for f in files if 'cat' in f]\n",
        "dog_files = [f for f in files if 'dog' in f]\n",
        "print(type(cat_files), len(cat_files), 'cats, ', len(dog_files), 'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we use np.random.choice which randomly selects `size` files and put them into the training set. Then the very clever `set` function which removes those files. Then do the same for validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pickout(source, size):\n",
        "    # choose elements without ever picking the same one twice\n",
        "    chosen = np.random.choice(source, size=size, replace=False)\n",
        "    remainder = list(set(source) - set(chosen))\n",
        "    return chosen, remainder\n",
        "\n",
        "# do it the rote way without a function\n",
        "cat_train = np.random.choice(cat_files, size=1500, replace=False)\n",
        "# convert two lists to sets, take the reminader and reconvert to list\n",
        "# https://www.geeksforgeeks.org/python-set-operations-union-intersection-difference-symmetric-difference/\n",
        "# minus means difference\n",
        "cat_less_train = list(set(cat_files) - set(cat_train))\n",
        "print('cat_train', len(cat_train), 'cat_less_train', len(cat_less_train))\n",
        "\n",
        "cat_val, cat_less_train_and_val = pickout(cat_less_train, 500)\n",
        "cat_test, _ = pickout(cat_less_train_and_val, 500)\n",
        "\n",
        "print('cat_train', len(cat_train), 'cat_val', len(cat_val), 'cat_test', len(cat_test))\n",
        "\n",
        "# now do the dogs\n",
        "\n",
        "dog_train, remainder = pickout(dog_files, 1500)\n",
        "dog_val, remainder = pickout(remainder, 500)\n",
        "dog_test, _ = pickout(remainder, 500)\n",
        "\n",
        "assert(set(dog_train).intersection(set(dog_val)) == set())\n",
        "assert(set(dog_train).intersection(set(dog_test)) == set())\n",
        "\n",
        "print('dog_train', len(dog_train), 'dog_val', len(dog_val), 'dog_test', len(dog_test))\n",
        "# https://stackoverflow.com/questions/6130374/empty-set-literal/6130391 \n",
        "# uses the method intersection and there is no literal for null\n",
        "# https://www.digitalocean.com/community/tutorials/how-to-use-args-and-kwargs-in-python-3\n",
        "# for variable arguments\n",
        "# https://stackoverflow.com/questions/22432814/check-if-a-collection-of-sets-is-pairwise-disjoint\n",
        "# this wrks by rebuilding element by element and ensuring there\n",
        "# are no duplicates which is the definition of all disjoing\n",
        "# note that this is not vectorized\n",
        "def all_disjoint(*args):\n",
        "    union = set()\n",
        "    for arg in args:\n",
        "        for item in arg:\n",
        "            if item in union:\n",
        "                # we found a duplicate so these are not all disjoing\n",
        "                return False\n",
        "            union.add(item)\n",
        "    return True\n",
        "\n",
        "assert(all_disjoint(dog_train, dog_val, dog_test, cat_train, cat_val, cat_test))\n",
        "# https://stackoverflow.com/questions/22432814/check-if-a-collection-of-sets-is-pairwise-disjoint\n",
        "# it's faster just to do a count and this is vectorized\n",
        "def all_disjoint_by_count(*args):\n",
        "    union_of_all_sets = set()\n",
        "    total_elements_of_all_sets = 0\n",
        "    for arg in args:\n",
        "        arg_as_set = set(arg)\n",
        "        union_of_all_sets = union_of_all_sets.union(arg_as_set)\n",
        "        total_elements_of_all_sets += len(arg_as_set)\n",
        "    return total_elements_of_all_sets == len(union_of_all_sets)\n",
        "\n",
        "assert(all_disjoint_by_count(dog_train, dog_val, dog_test, cat_train, cat_val, cat_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for convenience dump them all to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy_to_dir(target_dir, list of files...)\n",
        "# https://www.geeksforgeeks.org/args-kwargs-python/\n",
        "\n",
        "# warning you only need to run this once\n",
        "\n",
        "comment_out_this_if_you_really_need_for_the_first_time\n",
        "\n",
        "\n",
        "def copy_to_dir(target_dir, source, *args):\n",
        "    for arg in args:\n",
        "        source = np.concatenate([source, arg])\n",
        "    print('target_dir', target_dir, 'source', len(source))\n",
        "    # I have no idea why this works, but it is in \n",
        "    os.mkdir(target_dir) if not os.path.isdir(target_dir) else None\n",
        "    for file in source:\n",
        "        shutil.copy(file, target_dir)\n",
        "    \n",
        "\n",
        "# let's do some directory munging\n",
        "train_dir = 'train_data'\n",
        "val_dir = 'val_data'\n",
        "test_dir = 'test_data'\n",
        "\n",
        "# gether up all the files\n",
        "train_files = np.concatenate([cat_train, dog_train])\n",
        "print(type(train_files), train_files.shape)\n",
        "val_files = np.concatenate([cat_val, dog_val])\n",
        "\n",
        "# instead of that yack shaving, make a function\n",
        "copy_to_dir('train_data', cat_train, dog_train)\n",
        "copy_to_dir('val_data', cat_val, dog_val)\n",
        "copy_to_dir('test_data', cat_test, dog_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing Datasets\n",
        "Now the real work begin eparing the data sets. of pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Again we avoid duplication with a function\n",
        "# the default image size is 150x150 pixels\n",
        "# you can set the directory delimiter\n",
        "# And the file name delimiter\n",
        "def files_to_images_and_labels(source, img_dim=(150,150), dir_delimiter='/', file_delimiter='.'):\n",
        "    # assume a directory and wildcard everything there to get files\n",
        "    filenames = glob.glob(source + '/*')\n",
        "    print('source', source, 'filenames', len(filenames))\n",
        "    # use a list comprehension across all the files names\n",
        "    \n",
        "    images = [img_to_array(load_img(img, target_size=img_dim)) for img in filenames]\n",
        "    # in the tutorial, this is a numpy array, but as of June 2019\n",
        "    # this is just a plain list, so convert it\n",
        "    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.asarray.html\n",
        "    images = np.asarray(images)\n",
        "    \n",
        "    print('got images', len(images), type(images))\n",
        "    # now parse the lables, this is encoded in the filename like this\n",
        "    # /dir/../dir/label.number.ext\n",
        "    # so for instance the file name will be fully qualified\n",
        "    # so get rid of the slashes and the stuff after the dot which\n",
        "    # is the file type and number\n",
        "    # split breaks up the each file name by the delimiter.\n",
        "    # we want the very last one, by convention this is -1 the first on\n",
        "    # from the right\n",
        "    # then take this basename and pick the first one before the dot\n",
        "    # and strip out any white space\n",
        "    # use a list comprehension, which takes a list filenames and then \n",
        "    # processes each one and then puts it back into another list\n",
        "    labels = [fn.split(dir_delimiter)[-1].split(file_delimiter)[0].strip() for fn in filenames]\n",
        "    print('got labels', len(labels), type(labels))\n",
        "    return images, labels\n",
        "\n",
        "print('train_data')\n",
        "train_imgs, train_labels = files_to_images_and_labels('train_data')\n",
        "print('starting validation set')\n",
        "val_imgs, val_labels = files_to_images_and_labels('val_data')\n",
        "print('Train shape', train_imgs.shape, 'Train labels', len(train_labels))\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scaling and preparing the images\n",
        "Nwo we have 150 height x 150 width x 3 channels of RGB. So to get a better gradient scale the color channels from 0-255 to 0.0 to 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert from the integers to floating point\n",
        "train_imgs_scaled = train_imgs.astype('float32')\n",
        "val_imgs_scaled = val_imgs.astype('float32')\n",
        "\n",
        "# now scale from 0 to 1\n",
        "train_imgs_scaled /= 255\n",
        "val_imgs_scaled /= 255\n",
        "\n",
        "# Now debug it and take a look\n",
        "print(train_imgs[0].shape)\n",
        "array_to_img(train_imgs[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now get the labels ready as a one-hot vector\n",
        "We are using the SKlearn LabelEncoder to make this easy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "batch_size = 30\n",
        "num_classes = 2\n",
        "# Don't need so many epochs to prove the point\n",
        "# epochs = 30\n",
        "epochs = 10\n",
        "input_shape = (150, 150, 3)\n",
        "\n",
        "# Instantiate an encoder, not this is not a one-hot encoder\n",
        "le = LabelEncoder()\n",
        "# Go through all the training labels and learn the\n",
        "# different tags there\n",
        "le.fit(train_labels)\n",
        "# Now turn train and validation sets into one hot vecotrs\n",
        "train_labels_enc = le.transform(train_labels)\n",
        "val_labels_enc = le.transform(val_labels)\n",
        "\n",
        "# now let see what we have\n",
        "print('raw labels', train_labels[0:4], 'one-hot',  train_labels_enc[0:4])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple CNN from scratch\n",
        "Ok, let's build a simple CNN that uses the Sequential simple formulation. Ths is first subsample, then maxpool, then subsample and finish with a fully connected layer. \n",
        "\n",
        "See https://jovianlin.io/keras-models-sequential-vs-functional/ for the difference, but Deeplearning.ai teaches the functional model because it is more general and you can use for LSTMs, etc.\n",
        "\n",
        "The major difference is that instead of declaring a Sequential and starting, you instead declare inputs and then pass then as parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "\n",
        "# We are using the Sequential model for simplictity\n",
        "# but we should transform this to the API model at some point\n",
        "def sequential_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # convolve\n",
        "    model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "    # shrink the model by pooling a 2x2 into a 1x1\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # now convolve again and pool twice more\n",
        "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # convolve, pool and increase the channels again\n",
        "    model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "    # model.add(Conv2D(128, kernel_size(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # Now flatten the whole thing into a single vector\n",
        "    model.add(Flatten())\n",
        "    # Have afully connected layer so you have a 512 activation vector\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    \n",
        "    # Now stick a classifier on the bottom\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# https://jovianlin.io/keras-models-sequential-vs-functional/\n",
        "def functional_model(input_shape):\n",
        "    # The input is a image of height, width and RGB channels \n",
        "    inputs = Input(shape=input_shape)\n",
        "    # You can then pass it on to the next layer\n",
        "    # if they are not sharing weights, then you can just reuse a\n",
        "    # variable\n",
        "    X = Conv2D(16, kernel_size=(3,3), activation='relu')(inputs)\n",
        "    X = MaxPooling2D(pool_size=(2,2))(X)\n",
        "    X = Conv2D(64, kernel_size=(3,3), activation='relu')(X)\n",
        "    X = MaxPooling2D(pool_size=(2,2))(X)\n",
        "    X = Conv2D(128, kernel_size=(3,3), activation='relu')(X)\n",
        "    X = MaxPooling2D(pool_size=(2,2))(X)\n",
        "    \n",
        "    # Now flatten, full connect and add classifier\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(512, activation='relu')(X)\n",
        "    outputs = Dense(1, activation='sigmoid')(X)\n",
        "    \n",
        "    # note that unlike sequential, the model let's you \n",
        "    # set more than one input and more than one output\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_model = sequential_model(input_shape)\n",
        "\n",
        "# binary cross entropy is used for logistic regression\n",
        "seq_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(),\n",
        "                  metrics=['accuracy'])\n",
        "seq_model.summary()\n",
        "\n",
        "# this should be the same model, just different syntz\n",
        "func_model = functional_model(input_shape)\n",
        "\n",
        "func_model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "func_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are three convolutional layers. The flatten takes the 17x17 feature maps. and we now will run fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# because this is sequential, you just fill in the x and \n",
        "# this will take about 30 minutes on a Macbook using its CPU\n",
        "# under docker\n",
        "seq_history = seq_model.fit(x=train_imgs_scaled, y=train_labels_enc,\n",
        "                       validation_data=(val_imgs_scaled, val_labels_enc),\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=epochs,\n",
        "                       verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is overfitting. .99 accuracy on test and 67% on validation set. Now let's see how to do the same thing with the functional API and it has the same result which makes sense given these are the same models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://keras.io/models/model/\n",
        "# so fitting is exactly the same\n",
        "func_history = func_model.fit(x=train_imgs_scaled, y=train_labels_enc,\n",
        "                              validation_data=(val_dja;sdfkjas;dflkjas;dfljkimgs_scaled, val_labels_enc),\n",
        "                              batch_size=batch_size,\n",
        "                              epochs=epochs,\n",
        "                              verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use matplotlib to print a pair of two axies charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# plot a chart\n",
        "def perf_plot(axis, epochs, seq_history, category):\n",
        "    # now the first enumerates the epochs\n",
        "    epoch_list = list(range(1, epochs+1))\n",
        "    \n",
        "    # Now for the first subplot show the accuracies\n",
        "    axis.plot(epoch_list, seq_history.history[category], label='Train '+ category)\n",
        "    axis.plot(epoch_list, seq_history.history['val_' + category], label='Validation '+  category)\n",
        "\n",
        "    # now style the two \n",
        "    axis.set_xticks(np.arange(0, epochs + 1, 5))\n",
        "    axis.set_ylabel(category + ' Value')\n",
        "    axis.set_xlabel('Epoch')\n",
        "    axis.set_title(category + ' over Epochs')\n",
        "    legend = axis.legend(loc=\"best\")\n",
        "    return legend\n",
        "\n",
        "# create two subplots\n",
        "def plot_model(history):\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
        "    t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
        "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
        "    l1 = perf_plot(ax1, epochs, history, 'acc')\n",
        "    l2 = perf_plot(ax2, epochs, history, 'loss')\n",
        "    \n",
        "plot_model(seq_history)\n",
        "plot_model(func_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Adding regularization with dropouts\n",
        "The above plots are the classic example of overfitting. The Training accuracy improves but the validation accurcy stays the same. And the losses for validation actually increase over time.\n",
        "\n",
        "Using a dropout reduces overfitting on the training set. We will also try L2 regularization when this \n",
        "is verified to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now create a model with dropouts after each\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "language": "python",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
